\documentclass[12pt,a4paper]{report}

%====En-tête====
% Ajout des packages
\usepackage[french]{babel} % pour dire que le texte est en francais
\usepackage{a4} % pour la taille
\usepackage[T1]{fontenc} % pour les font postscript
\usepackage[cyr]{aeguill} % Police vectorielle TrueType, guillemets francais
\usepackage{epsfig} % pour gérer les images
\usepackage{amsmath,amsthm} % très bon mode mathématique
\usepackage{amsfonts,amssymb,bm, bbold}% permet la definition des ensembles
\usepackage{algorithm2e} % pour les algorithmes
\usepackage{algpseudocode} % pour les algorithmes
\usepackage{float} % pour le placement des figure
\usepackage{url} % pour une gestion efficace des url
\usepackage{hyperref} % pour les hyperliens dans le document
\usepackage{tocbibind} % Pour avoir des index pour table des matières, biblio

% Nouvelles commandes
\newcommand{\Tau}{\mathcal{T}}

% titre et auteur
\title{Rapport de stage dans l'UMR MIA Paris-Saclay}
\author{Louis Lacoste}

\begin{document}
\maketitle
\tableofcontents

\chapter{Présentation de l'UMR}

\chapter{Context}

Develop on what is the SBM, colSBM, etc.

\chapter{Adjustment of colSBM to the bipartite case: colBiSBM}

\section{Variational Expectation step}
Fixed point formula for the Bernoulli distribution:
% Repasser à l'exponentielle pour la présentation du point fixe
\begin{itemize}
    \item \textit{iid} :
    \[ \bm{\tau}^{m,1} = ~^{t}\pi + \exp[(\text{Mask}^{m} \odot A^{m})
        \bm{\tau}^{m,2} ~^{t}(\text{logit}(\alpha)) + \text{Mask}^{m} 
        \bm{\tau}^{m,2} ~^{t}\log(\bm{1} - \alpha)] \]
    \[ \log(\bm{\tau}^{m,2}) = ~^{t}\log(\rho) + ~^{t}(\text{Mask}^{m} \odot A^{m}) 
    \bm{\tau}^{m,1} \text{logit}(\alpha) + ~^{t}\text{Mask}^{m} 
        \bm{\tau}^{m,1} \log(\bm{1} - \alpha) \]
    \item $\rho\pi$ :
        \[ \log(\bm{\tau}^{m,1}) = ~^{t}\log(\pi^{m}) + (\text{Mask}^{m} \odot A^{m})
            \bm{\tau}^{m,2} ~^{t}(\text{logit}(\alpha)) + \text{Mask}^{m} 
            \bm{\tau}^{m,2} ~^{t}\log(\bm{1} - \alpha) \]
        \[ \log(\bm{\tau}^{m,2}) = ~^{t}\log(\rho^{m}) + ~^{t}(\text{Mask}^{m} \odot A^{m}) 
        \bm{\tau}^{m,1} \text{logit}(\alpha) + ~^{t}\text{Mask}^{m} 
            \bm{\tau}^{m,1} \log(\bm{1} - \alpha) \]
\end{itemize}

with $\text{Mask}^{m}$ the matrix containing $0$ if the value is a NA and a 1
otherwise.

\section{M step of the algorithm}

Incorporate the equations from \cite{chabert-liddellLearningCommonStructures2023}

\section{Computation of the variational bound}

\section{Penalties}

\paragraph*{\textit{iid-colBiSBM}}
For the \textit{iid-colBiSBM} the penalties were modified in the following way :

\begin{itemize}
    \item For the $\pi$s and $\rho$s:
    \[\text{pen}_{\pi}(Q_1) = (Q_1 - 1)\log(\sum_{m=1}^{M}n_{r}^{(m)})\]
    \[\text{pen}_{\rho}(Q_2) = (Q_2 - 1)\log(\sum_{m=1}^{M}n_{c}^{(m)})\]
    \item For the $\alpha$s :
    \[\text{pen}_{\alpha}(Q_1, Q_2) = Q_1 \times Q_2 \log(N_M)\]
    avec
    \[ N_M = \sum_{m = 1}^{M} n_{r}^{(m)} \times n_{c}^{(m)} \]
\end{itemize}
And thus the $\text{BIC-L}$ formula is now:
\[ \text{BIC-L}(\bm{X},Q_1, Q_2) = \max_{\theta} \mathcal{J} (\mathcal{\hat{R}}, \bm{\theta}) 
- \frac{1}{2} [\text{pen}_{\pi}(Q_1) + \text{pen}_{\rho}(Q_2) + \text{pen}_{\alpha}(Q_1, Q_2)]\]

\paragraph*{\textit{$\rho\pi$-colBiSBM}}
For the \textit{$\rho\pi$-colBiSBM} the penalties are the following:

\begin{itemize}
    \item The support penalties are:
    \[ \text{pen}_{S_1}(Q_1) = -2 \log p_{Q_1} (S_1) \]
    \[ \text{pen}_{S_2}(Q_2) = -2 \log p_{Q_2} (S_2) \]
    with
    \[ \log p_{Q_1}(S_1) = - M \log(Q_1) - \sum_{m=1}^{M} \log {Q_1 \choose Q_1^{(m)}} \]
    \[ \log p_{Q_2}(S_2) = - M \log(Q_2) - \sum_{m=1}^{M} \log {Q_2 \choose Q_2^{(m)}} \]
    \item Penalties for the $\rho$s and $\pi$s:
    \[ \text{pen}_{\pi}(Q_1, S_1) = \sum_{m=1}^{M} (Q_{1}^{(m)} - 1) \log n_{r}^{(m)} \]
    \[ \text{pen}_{\rho}(Q_2, S_2) = \sum_{m=1}^{M} (Q_{2}^{(m)} - 1) \log n_{c}^{(m)} \]
    \item Penalties for the $\alpha$s:
    \[ \text{pen}_{\alpha}(Q_1, Q_2, S_1, S_2) = (\sum_{q=1}^{Q_1} \sum_{r=1}^{Q_2} \mathbb{1}_{(S_1)'S_2 > 0}) \log (N_M) \]
\end{itemize}
And the corresponding BIC-L formula:
\[
    \begin{aligned}
        \text{BIC-L}(\bm{X},Q_1, Q_2) = 
        \max_{S_1,S_2} [
            & \max_{\theta_{S_1,S_2} \in \Theta_{S_1,S_2}} \mathcal{J}(\mathcal{\hat{R}},\theta_{S_1,S_2})\\
            - \frac{1}{2} & (\text{pen}_{\pi}(Q_1, S_1)  + \text{pen}_{\rho}(Q_2, S_2)\\
            &+ \text{pen}_{\alpha}(Q_1, Q_2, S_1, S_2)\\
            &+ \text{pen}_{S_1}(Q_1) + \text{pen}_{S_2}(Q_2))]\\
    \end{aligned}
\]

\section{Latent space exploration and model selection}
In order to explorer the bi-dimensional latent space $(Q_1,Q_2)$
we use the following strategies.

\subsection{Model selection}
In the following steps the model selection consists of using the BIC-L 
criterion to select the model. We choose among the proposed models the one that
maximizes the BIC-L

\subsection{Initialization and pairing of the models}
First to combine the information from the $M$ networks we fit a collection model
for each network at the two points $Q = (1, 2)$ and $Q = (2, 1)$. Using the 
previously described VEM algorithm we obtain for each network its parameters 
($\rho,\pi,\alpha$).

We then compute the marginal laws for each dimension, for each network. Then 
we order the network blocks by the probabilities obtained in decreasing order.
\begin{itemize}
    \item For the memberships on the columns: 
    $col~order_m = order\left(\pi_m \times \alpha_m\right)$ 
    \item For the memberships on the rows:
    $row~order_m = order\left(\rho_m \times ~^{t}(\alpha_m)\right)$ 
\end{itemize}

Using this order we relabel the memberships for the $M$ fitted collection of a
single network.
Then we use the $M$ memberships to fit a collection containing the $M$ networks.
\subsection{Greedy exploration to find an estimation of the mode}
Using the previously fitted models for $Q = (1,2)$ and $Q = (2,1)$ we choose to
perform a greedy exploration to find a first mode.

Meaning that for a given $Q = (Q_1, Q_2)$ we will compute all the possible 
memberships for the points $Q \in \{(Q_1 + 1, Q_2),(Q_1, Q_2 + 1),(Q_1 - 1, Q_2),
(Q_1, Q_2 - 1)\}$, fit
the corresponding models and choose the one that maximizes the BIC-L as the 
next point from which to repeat the procedure. We repeat the procedure until the
BIC-L stops increasing $2$ times in a row.

\begin{algorithm}[H]
    \caption{Greedy Exploration for Mode Estimation}
    \SetAlgoLined
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    
    \Input{Fitted models for $Q = (1,2)$ and $Q = (2,1)$}
    \Output{Estimation of the mode using greedy exploration}
    
    \BlankLine
    Initialize $Q = (1,2)$ as the starting point
    Initialize $\text{BIC-L}_{\text{max}}$ as the maximum achieved BIC-L value
    Initialize $consecutive\_count$ as 0
    
    \BlankLine
    \While{$consecutive\_count < 2$}{
        Compute possible memberships for $Q \in \{(Q_1 + 1, Q_2), (Q_1, Q_2 + 1), (Q_1 - 1, Q_2), (Q_1, Q_2 - 1)\}$\;
        Fit models with the computed memberships
        Choose the model with the maximum BIC-L as the next point
        
        \BlankLine
        \If{$\text{BIC-L} > \text{BIC-L}_{\text{max}}$}{
            $\text{BIC-L}_{\text{max}} \leftarrow \text{BIC-L}$
            $consecutive\_count \leftarrow 0$
        }
        \Else{
            $consecutive\_count \leftarrow consecutive\_count + 1$
        }
        
        \BlankLine
        $Q \leftarrow$ Next selected point
    }
    
    \BlankLine
    \textbf{Output:} Estimation of the mode using greedy exploration
\end{algorithm}

When this first estimation of the BIC-L mode has been find we apply the moving
window on it.
\subsection{Moving window to update the block memberships and the BIC-L}
The \emph{moving window} is used to update the block memberships on rows and 
columns and fit new models with those changes.
To define the window, we use a center point and a \emph{depth}, giving us the 
bottom left corner ($Q_{1,center} - depth, Q_{2,center} - depth$) and the top right corner of the 
window ($Q_{1,center} + depth, Q_{2,center} + depth$). All the points in this square will be 
updated and contribute to the update of the others.
This procedure is repeated until convergence of the BIC-L.

The procedure consists of two alternating steps:
\begin{itemize}
    \item the \emph{forward pass}: repeatedly computing the possible splits to 
    fit the current model.
    \item the \emph{backward pass}: computing the possible merges to fit the current model.
\end{itemize}


\begin{algorithm}[H]
    \caption{Moving Window Procedure}
    \SetAlgoLined
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \Input{Center point $(Q_{1,\text{center}}, Q_{2,\text{center}})$, depth}
    \Output{Best model with maximum BIC-L in the window}

    \BlankLine
    Define bottom left corner $(Q_{1,\text{center}} - \text{depth}, Q_{2,\text{center}} - \text{depth})$\\
    Define top right corner $(Q_{1,\text{center}} + \text{depth}, Q_{2,\text{center}} + \text{depth})$

    \BlankLine
    \While{not converged}{
        \textbf{Forward pass:}

        \For{$Q_1 \in \left[ Q_{1,\text{center}} - \text{depth} ; Q_{1,\text{center}} + \text{depth} \right]$}{
            \For{$Q_2 \in \left[ Q_{2,\text{center}} - \text{depth}; Q_{2,\text{center}} + \text{depth} \right] $}{
                Compute possible splits from predecessors $(Q_1 - 1, Q_2)$ and $(Q_1, Q_2 - 1)$
                Fit models with the block membership changes
                Compare and keep the best model based on BIC-L
            }
        }

        \BlankLine
        \textbf{Backward pass:}

        \For{$Q_1 \in \left[ Q_{1,\text{center}} + \text{depth} ; Q_{1,\text{center}} - \text{depth} \right]$}{
            \For{$Q_2 \in \left[ Q_{2,\text{center}} + \text{depth}; Q_{2,\text{center}} - \text{depth} \right] $}{
                Compute possible merges from predecessors $(Q_1 + 1, Q_2)$ and $(Q_1, Q_2 + 1)$
                Fit models with the block membership changes
                Compare and keep the best model based on BIC-L
            }
        }

        \BlankLine
        Update the best model based on the maximum BIC-L
    }

    \BlankLine
    \textbf{Output:} Best model with maximum BIC-L in the window
\end{algorithm}


\paragraph*{Forward pass} The forward pass consists for a model at $(Q_1, Q_2)$ 
to compute the possible splits from the block memberships of its "predecessors". 
The predecessors are the point at the left $(Q_1 - 1, Q_2)$ and below 
$(Q_1, Q_2 - 1)$ the current model (if they exist). To update the current model,
we take its predecessors block memberships and try to split one of the blocks in
two. Then the current model is fitted using this clustering as a starting 
clustering. Once all the possible splits are fitted, they are compared, keeping 
the best, in the sense of the BIC-L. If a model was already present it is also
compared and the best is chosen as the model for this round at $(Q_1, Q_2)$.\\
The procedure then repeats for the point at $(Q_1 + 1, Q_2)$ until it reaches 
$(Q_{1,center} + depth, Q_2)$ from which it repeats from 
$(Q_{1,center} - depth, Q_2 + 1)$. This repeats until computing the best model
for $(Q_{1,center} + depth, Q_{2,center} + depth)$.
\textit{Note on the initialization:} The forward pass starts from the point 
$(Q_{1,center} + depth, Q_{2,center} + depth)$, so this points needs to have at 
least a model fitted. In the best case, the greedy exploration will have visited
this point. But if the point has not been visited, a model will be fitted from
a spectral initialization (i.e the block memberships is computed by using a 
spectral clustering). From this point, the next model will have at least one
predecessor and the procedure can iterate.

\paragraph*{Backward pass} The backward pass consists for a model at $(Q_1, Q_2)$ 
to compute the possible merges from the block memberships of its "predecessors". 
The predecessors are the point at the right $(Q_1 + 1, Q_2)$ and on top 
$(Q_1, Q_2 + 1)$ of the current model (if the predecessors exist). To update the
current model, we take its predecessors block memberships and try to merge two 
blocks in one. Then the current model is fitted using this clustering as
a starting clustering. Once all the possible merges are fitted, they are
compared, keeping the best, in the sense of the BIC-L.
If a model was already present it is also
compared and the best is chosen as the model for this round at $(Q_1, Q_2)$.\\
The procedure then repeats for the point at $(Q_1 - 1, Q_2)$ until it reaches 
$(Q_{1,center} - depth, Q_2)$ from which it repeats from 
$(Q_{1,center} - depth, Q_2 - 1)$. This repeats until computing the best model
for ($Q_{1,center} - depth, Q_{2,center} - depth$).
\textit{Note on the initialization:} The backward pass starts from
$(Q_{1,center} + depth, Q_{2,center} + depth)$, we know it was initialized at
least by the forward pass, no special case here.\\

At the end of the moving window pass, the model of max BIC-L is the new best
fit and the procedure can repeat until convergence.

\section{Networks clustering}
As in \cite{chabert-liddellLearningCommonStructures2023} we use a recursive
algorithm to determine the best clustering of the given networks. The procedure
being the same, only the technical modifications for the bipartite case will be
explained below.
\subsection{Distance between two networks}
The distance weights uses $\pi$ and $\rho$.
\[ 
    D_{\mathcal{M}}(m,m') = \sum_{q = 1}^{Q_1} \sum_{r = 1}^{Q_2} \max(\widetilde{\pi}_{q}^{m}, \widetilde{\pi}_{q}^{m'}) \left( \frac{\widetilde{\alpha}_{qr}^{m}}{\widehat{\delta}_{m}} - \frac{\widetilde{\alpha}_{qr}^{m'}}{\widehat{\delta}_{m'}}\right)^{2} \max(\widetilde{\rho}_{r}^{m}, \widetilde{\rho}_{r}^{m'}) 
\]


\nocite{*}
\bibliographystyle{plain}
\bibliography{references.bib}
\listoffigures
\listoftables
\end{document}